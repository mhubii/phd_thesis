% \subsubsection{Point-to-plane ICP: A Lie Algebra Formulation}
% \label{in:sec:p2plane}

\paragraph{Basic Formulation}
We first introduce the problem solved by the point-to-plane \gls{icp} algorithm~\cite{Rusinkiewicz:IC3DIM:2001}.
Let $\mathbf{p}_i$ and $\mathbf{q}_i$ be corresponding 3D points in homogeneous coordinates. Let $\mathbf{n}_i$ be a normal vector associated with $\mathbf{q}_i$. We search a rigid body transformation $\mathbf{\Theta}$ in $SE(3)$ that minimizes:
\begin{equation}
\sum_i ||\mathbf{n}_i^T \cdot (\mathbf{\Theta} \cdot \mathbf{p}_i - \mathbf{q}_i)||^2 = \sum_i (\mathbf{n}_i^T \cdot \mathbf{\Theta} \cdot \mathbf{p}_i - \mathbf{n}_i^T \cdot \mathbf{q}_i)^2
\end{equation}

\paragraph{Iterative Optimization on the $SE(3)$ Lie Group}
Let $\mathbf{\Theta}_0$ be the current estimate. In a Lie group iterative optimization approach~\cite{Mahony:JGO:2002,Vercauteren:IPMI:2007}, we seek a perturbation $\Delta\mathbf{\Theta} \in \mathfrak{se}(3)$ composed with
$\mathbf{\Theta}_0$: $\mathbf{\Theta}_1 = \mathbf{\Theta}_0 \cdot \exp(\Delta\mathbf{\Theta})$. We now seek to minimize:
\begin{equation}
\sum_i (\mathbf{n}_i^T \cdot \mathbf{\Theta}_0 \cdot \exp(\Delta\mathbf{\Theta}) \cdot \mathbf{p}_i - \mathbf{n}_i^T \cdot \mathbf{q}_i)^2
\end{equation}

\paragraph{Mathematical Preliminaries}
For convenience, we take advantage of the relationship between the 3D cross product, the Lie bracket on SO(3), and the skew-symmetric matrix operator.
Let $\boldsymbol{\omega} = [\omega_x, \omega_y, \omega_z]^T$ and $\boldsymbol{\eta} = [\eta_x, \eta_y, \eta_z]^T$ be two 3D vectors, we have
\begin{equation}
[\boldsymbol{\omega}, \boldsymbol{\eta}] = \boldsymbol{\omega} \times \boldsymbol{\eta} = \boldsymbol{\omega}_\times \boldsymbol{\eta}
\end{equation}
where
\begin{equation}
\boldsymbol{\omega}_\times =
\begin{bmatrix}
0         & -\omega_z & \omega_y \\
\omega_z  & 0         & -\omega_x \\
-\omega_y & \omega_x  & 0
\end{bmatrix}
\end{equation}

It can be shown that the Lie group exponential in SO(3) admits a closed form through the Rodriguesâ€™ formula. Let us consider $\boldsymbol{\omega}$ as an element of $\mathfrak{so}(3)$, we now have
\begin{equation}
\expl(\boldsymbol{\omega}) = \exp(\boldsymbol{\omega}_\times) = \mathbf{I}^{3\times3} + \frac{\sin(||\boldsymbol{\omega}||)}{||\boldsymbol{\omega}||}\boldsymbol{\omega}_\times + \frac{1-\cos(||\boldsymbol{\omega}||)}{||\boldsymbol{\omega}||^2}\boldsymbol{\omega}_\times^2
\end{equation}
which for small values of $||\boldsymbol{\omega}||$ leads to the following numerically stable approximation
\begin{equation}
\expl(\boldsymbol{\omega}) \approx \mathbf{I}^{3\times3}
+ (1-\frac{||\boldsymbol{\omega}||^2}{6}+\frac{||\boldsymbol{\omega}||^4}{120})\boldsymbol{\omega}_\times
+ (\frac{1}{2}-\frac{||\boldsymbol{\omega}||^2}{24}+\frac{||\boldsymbol{\omega}||^4}{720}))\boldsymbol{\omega}_\times^2
\end{equation}

\iffalse
For compatibility with the 3D cross product, we choose the following basis for the matrix representation of $\mathfrak{se}(3)$:
\begin{align}
e_1 &= 
\begin{bmatrix}
0 & 0 & 0  & 0 \\
0 & 0 & -1 & 0 \\
0 & 1 & 0  & 0 \\
0 & 0 & 0  & 0
\end{bmatrix}
\\
e_2 &=
\begin{bmatrix}
0  & 0 & 1 & 0 \\
0  & 0 & 0 & 0 \\
-1 & 0 & 0 & 0 \\
0  & 0 & 0 & 0
\end{bmatrix}
\\
e_3 &=
\begin{bmatrix}
0 & -1 & 0 & 0 \\
1 & 0  & 0 & 0 \\
0 & 0  & 0 & 0 \\
0 & 0  & 0 & 0
\end{bmatrix}
\\
e_4 &=
\begin{bmatrix}
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
\\
e_5 &=
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
\\
e_6 &=
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{bmatrix}
\end{align}
\fi

Let us consider a 6D vector $\mathbf{u}=[\boldsymbol{\omega};\boldsymbol{\tau}]$ as an element of $\mathfrak{se}(3)$. Its matrix representation is provided by
\begin{equation}
  \mathbf{u}_\dagger =
\begin{bmatrix}
\boldsymbol{\omega}_\times & \boldsymbol{\tau} \\
0  & 0
\end{bmatrix} \in \mathbb{R}^{4\times4}
\end{equation}
which leads to the following closed-form formula for the Lie group exponential for SE(3):
\begin{equation}
\expl(\mathbf{u}) = \exp(\mathbf{u}_\dagger) =
\begin{bmatrix}
\expl(\boldsymbol{\omega}) & L(\boldsymbol{\omega})\boldsymbol{\tau} \\
0  & 1
\end{bmatrix}
\end{equation}
where
\begin{equation}
L(\boldsymbol{\omega}) = \mathbf{I}^{3\times3}
+ \frac{1-\cos(||\boldsymbol{\omega}||)}{||\boldsymbol{\omega}||^2}\boldsymbol{\omega}_\times
+ \frac{||\boldsymbol{\omega}||-\sin(||\boldsymbol{\omega}||)}{||\boldsymbol{\omega}||^3}\boldsymbol{\omega}_\times^2
\end{equation}
where a Taylor expansion should again be used for numerical stability if $||\boldsymbol{\omega}||$ is small:
\begin{equation}
L(\boldsymbol{\omega}) \approx \mathbf{I}^{3\times3}
+ (\frac{1}{2}-\frac{||\boldsymbol{\omega}||^2}{24}+\frac{||\boldsymbol{\omega}||^4}{720})\boldsymbol{\omega}_\times
+ (\frac{1}{6}-\frac{||\boldsymbol{\omega}||^2}{120}+\frac{||\boldsymbol{\omega}||^4}{5040})\boldsymbol{\omega}_\times^2
\end{equation}

\paragraph{First-order Linearization of the Action of the Exponential in $\mathfrak{se}(3)$}
Let $\mathbf{p}=[\tilde{\mathbf{p}};1]$ be a 3D point in homogeneous coordinates. We consider an infinitesimal element $\Delta \mathbf{u}=[\Delta \boldsymbol{\omega};\Delta \boldsymbol{\tau}]$ of $\mathfrak{se}(3)$ and consider the corresponding action on $p$:
\begin{equation}
\begin{split}
\expl(\Delta \mathbf{u})\mathbf{p} &\approx (\mathbf{I}^{4\times4}+\Delta \mathbf{u}_\dagger)\mathbf{p} = \mathbf{p} +
\begin{bmatrix}
\Delta \boldsymbol{\omega}_\times & \Delta \boldsymbol{\tau} \\
0  & 0
\end{bmatrix} \mathbf{p}
= \mathbf{p} +
\begin{bmatrix}
\Delta \boldsymbol{\omega}_\times \tilde{\mathbf{p}}+\Delta \boldsymbol{\tau} \\
0
\end{bmatrix} \\
&= \mathbf{p} +
\begin{bmatrix}
\Delta \boldsymbol{\omega} \times \tilde{\mathbf{p}}+\Delta \boldsymbol{\tau} \\
0
\end{bmatrix}
= \mathbf{p} +
\begin{bmatrix}
-\tilde{\mathbf{p}} \times \Delta \boldsymbol{\omega} + \Delta \boldsymbol{\tau} \\
0
\end{bmatrix} \\
&= \mathbf{p} +
\begin{bmatrix}
-\tilde{\mathbf{p}}_\times & \mathbf{I}^{3\times3} \\
0 & 0
\end{bmatrix} \Delta \mathbf{u}
= \mathbf{p} + \mathbf{D}(\mathbf{p})\Delta \mathbf{u}
\end{split}
\end{equation}
where
\begin{equation}
\mathbf{D}(\mathbf{p}) = 
\begin{bmatrix}
-\tilde{\mathbf{p}}_\times & \mathbf{I}^{3\times3} \\
0 & 0
\end{bmatrix} \in \mathbb{R}^{4\times6}
\end{equation}

\paragraph{Lie Algebra Linearization of the Point-to-plane Objective}
Plugging this in the original cost function we get the following linearization:
\begin{equation}
\begin{split}
\sum_i & \Big(\mathbf{n}_i^T \mathbf{\Theta}_0 \big(\mathbf{p}_i + \mathbf{D}(\mathbf{p}_i)\Delta\mathbf{\Theta} \big) - \mathbf{n}_i^T \mathbf{q}_i\Big)^2
\\
&= \sum_i \Big(\mathbf{n}_i^T \mathbf{\Theta}_0 \mathbf{D}(\mathbf{p}_i)\Delta\mathbf{\Theta} + \mathbf{n}_i^T \mathbf{\Theta}_0 \mathbf{p}_i - \mathbf{n}_i^T \mathbf{q}_i\Big)^2
\end{split}
\end{equation}
Denoting $\mathbf{a}_i = \mathbf{n}_i^T \mathbf{\Theta}_0 \mathbf{D}(\mathbf{p}_i)$, $\mathbf{A}=[\mathbf{a}_0;\ldots;\mathbf{a}_N]$,
$b_i = \mathbf{n}_i^T (\mathbf{q}_i - \mathbf{\Theta}_0 \mathbf{p}_i)$, and $\mathbf{B}=[b_0;\ldots;b_N]$, we end up with a standard linear least squares problem:
\begin{equation}
||\mathbf{A} \Delta\mathbf{\Theta} - \mathbf{B}||^2
\end{equation}
which can be expressed using the pseudo-inverse $\mathbf{A}^{\dagger}$ of $\mathbf{A}$:
\begin{equation}
\Delta\mathbf{\Theta} = \mathbf{A}^{\dagger} \mathbf{B}
\end{equation}

Given the dimensions of $\mathbf{A}$, $b$ and $\Delta\mathbf{\Theta}$ is probably best solved by using the normal equations
\begin{equation}
(\mathbf{A}^T \mathbf{A}) \Delta\mathbf{\Theta} = \mathbf{A}^T \mathbf{B}
\end{equation}
and relying on a Cholesky decomposition:
\begin{equation}
\Delta\mathbf{\Theta} = \texttt{chol\_solve}(\mathbf{A}^T \mathbf{A}, \mathbf{A}^T \mathbf{B})
\end{equation}

Note that by expressing $\mathbf{A}$ and $\mathbf{B}$ without homogeneous coordinates, we get:
\begin{equation}
\mathbf{a}_i = [-\tilde{\mathbf{n}}_i^T \mathbf{R}_0 {{\tilde{\mathbf{p}}}_{i\times}}, \tilde{\mathbf{n}}_i^T \mathbf{R}_0] \in \mathbb{R}^{1\times6}
\end{equation}
and
\begin{equation}
b_i = \tilde{\mathbf{n}}_i^T (\tilde{\mathbf{q}}_i - \mathbf{R}_0 \tilde{\mathbf{p}}_i - T_0) \in \mathbb{R}
\end{equation}

\paragraph{Robust Formulation}
To reduce the influence of outliers on the solution, a typical approach is to introduce a
loss function $\rho_\kappa$ parameterized by a scale parameter (a.k.a. soft margin or cut-off) $\kappa$ to scale the residuals. We then seek to minimize:
\iffalse
\begin{equation}
\sum_i \rho\big(\frac{1}{C^2}(\mathbf{n}_i^T \cdot \mathbf{\Theta} \cdot \mathbf{p}_i - \mathbf{n}_i^T \cdot \mathbf{q}_i)^2\big)
\end{equation}

Using the identity loss function $\rho(z)=z$ leads to the original least-squares problem. Classical robust alternatives include:
\begin{description}
  \item[Soft L1]:
  $\rho(z) = 2 (\sqrt{1 + z} - 1)$.
  The smooth approximation of l1 (absolute value) loss. Usually a good choice for robust least squares.

  \item[Huber]:
  $\rho(z) = z \texttt{ if } z <= 1 \texttt{ else } 2\sqrt{z} - 1$.
  Works similarly to ``Soft L1".

  \item[Cauchy]:
  $\rho(z) = \ln(1 + z)$.
  Severely weakens outliers influence, but may cause difficulties in optimization process.

   \item[Arctan]:
   $\rho(z) = \arctan(z)$.
   Limits a maximum loss on a single residual, has properties similar to ``Cauchy".
\end{description}
\fi
\begin{equation}
\sum_i \rho_k\Big(\mathbf{n}_i^T \cdot \mathbf{\Theta} \cdot \mathbf{p}_i - \mathbf{n}_i^T \cdot \mathbf{q}_i\Big)
\end{equation}
or equivalently if we start from a given estimate in the above Lie group setting:
\begin{equation}
\sum_i \rho_k\Big(\mathbf{a}_i \Delta\mathbf{\Theta} - b_i\Big)
\end{equation}
%
Using the square loss function $\rho_\kappa(z)=z^2$ leads to the original least-squares problem. A classical robust alternatives is the Huber loss
\begin{equation}
\rho_\kappa(z) =
\begin{cases}
  z^2 &\texttt{ if } |z| <= \kappa \\
  2|z|\kappa - \kappa^2  &\texttt{ otherwise }
\end{cases}
\end{equation}
with $\kappa=1.345\sigma$ being advocated to provide robustness while retaining appropriate properties when the errors are Gaussian.

This minimisation can be achieved by a standard \gls{irls} approach~\cite{Green:JRSSB:1984} or by including a second order terms in a slightly more advanced reweighted Gauss-Newton approach as discussed in~\cite{Triggs:VisAlg:2000}.
The latter is referred to as the Triggs correction in \url{http://ceres-solver.org/nnls_modeling.html#theory}. While elegant, previous work has shown that the Triggs correction does not significantly improve on the standard \gls{irls}~\cite{Zach:ECCV:2014,Zach:ECCV:2018}.
Here, for simplicity we thus restrict ourselves to standard \gls{irls}.
Given a choice of scaling factor $\kappa$ and an estimate of the parameters $\mathbf{\Theta}$, the robust problem is converted to a weighted least-squares:
\begin{equation}
\sum_i \boldsymbol{\omega}_\kappa(b_i) ||\mathbf{a}_i \Delta\mathbf{\Theta} - b_i||^2
\end{equation}
with $\boldsymbol{\omega}_\kappa(z) = \rho'_\kappa(z) / z$ being the weight function associated with the Huber loss:
\begin{equation}
\boldsymbol{\omega}_\kappa(z) =
\begin{cases}
  1 &\texttt{ if } |z| <= \kappa \\
  \kappa / |z|  &\texttt{ otherwise }
\end{cases}
\end{equation}

This leads us to the following weighted normal equation:
\begin{equation}
(\mathbf{A}^T W \mathbf{A}) \Delta\mathbf{\Theta} = \mathbf{A}^T \mathbf{W} \mathbf{B}
\end{equation}
where $\mathbf{W} = \diag\big(\boldsymbol{\omega}_\kappa(b_0), \ldots, \boldsymbol{\omega}_\kappa(b_N)\big)$.

One aspect we haven't addressed yet it the choice of $\kappa$ for the Huber loss. As mentioned earlier, a typical choice is to use $\kappa=1.345\sigma$. We are thus left with the need to robustly estimate the standard deviation of the residuals. This is typically done through a \gls{mad}:
\begin{equation}
\hat{\sigma} = \frac{\MAD(\{b_i\})}{0.6745} = \frac{\median(\{|b_i-\median(\{b_i\}|)\})}{0.6745}
\end{equation}

%% unused section in PhD thesis
% \subsubsection{Registration of a 3D Point Cloud with 2D Stereo Segmentations}
% \label{in:sec:registration_of_3d_point_cloud}
% %
% Let $\mathbf{q}_i$ be 3D points associated to a model object as expressed in an object specific coordinate frame.
% Let $I_L$ and $I_R$ be a pair of stereo images with associated $M_L$ and $M_R$ camera matrices (including both extrinsic and intrinsic).
% We assume that the stereo images are observing the object in an unknown pose $\mathbf{\Theta}$.
% We also assume that the object is relatively textureless and that we do not have a good means of extracting keypoints of interest from the images.
% We however assume that a segmentation of the object in each image can readily be computed, e.g. with the \gls{sam}~\cite{Kirillov:arxiv:2023}.
% %
% While differentiable rendering could be used in this context~\cite{Hannemose:SPIE:2019,labbe2021single,Chen:RAL:2023},
% we expect that a simpler algorithm can be designed given the segmentation availability by matching points from 3D to 2D.
% We also expect to potentially gain robustness against illumination and other factors that would impact the appearance of the object in the images.

% Let $S_L$ and $S_R$ be the object segmentation masks in the left and right image respectively.
% We here seek to recover the pose $\mathbf{\Theta}$ such that the projected model points ${M_L \mathbf{\Theta} q}$ match the observed segmentation $S_L$ (respectively ${M_R \mathbf{\Theta} q}$ to match $S_R$).
% Let $\varphi$ be a function to transform from homogeneous to Cartesian coordinates.
% That is $\varphi([x,y,f])=[x/f,y/f]$ and similarly in 3D.
% We can write the objective as:
% \begin{equation}
% \arg\min_{\mathbf{\Theta}} \sum_i
% \min_{p_L|S_L(p_L)=1}|| \varphi\big(M_L \mathbf{\Theta} \mathbf{q}_i\big) - p_L||^2
% + \min_{p_R|S_R(p_R)=1}|| \varphi\big(M_R \mathbf{\Theta} \mathbf{q}_i\big) - p_R||^2
% \end{equation}

% Similar to the approach in~\cite{Fitzgibbon:IVC:2003}, this formulation can be substantially accelerated by taking advantage of the fact that the segmentations are defined on a regular grid. We can thus use a \gls{edt} for which optimized algorithms exist on both \gls{cpu}~\cite{Felzenszwalb:TC:2012}\footnote{\url{https://github.com/cai4cai/distance_transform}} and \gls{gpu}~\cite{Cao:SIGGRAPH:2010}\footnote{\url{https://docs.rapids.ai/api/cucim/stable/api/\#cucim.core.operations.morphology.distance_transform_edt}}.
% Let $\mathcal{D}_{S_L}$ be the \gls{edt} computed from $S_L$ (respectively $\mathcal{D}_{S_R}$ for $S_R$).
% We can now write
% \begin{equation}\label{eq:dt3d2d}
% \arg\min_{\mathbf{\Theta}} \sum_i \mathcal{D}_{S_L}^2\big( \varphi(M_L \mathbf{\Theta} \mathbf{q}_i) \big)
% + \mathcal{D}_{S_R}^2\big( \varphi(M_R \mathbf{\Theta} \mathbf{q}_i) \big)
% \end{equation}

% As in \secref{in:sec:p2plane}, the square loss terms could be replaced by robust alternatives.
% It should further be noted that the evaluation of a distance transform $\mathcal{D}_{S}$ at an arbitrary point $p$ requires the use of an interpolator.
% A typical choice is to rely on bilinear interpolation for which optimized implementations exist on both \gls{cpu} and \gls{gpu}\footnote{\url{https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html}}.

% Solving for \eqref{eq:dt3d2d} can be done again by taking advantage of the Lie group structure of SE(3). For simplicity, a generic Lie group non-linear least-squares optimizer as found in Theseus\footnote{\url{https://github.com/facebookresearch/theseus}}~\cite{Pineda:NeurIPS:2023} or Ceres\footnote{\url{https://github.com/ceres-solver/ceres-solver}}~\cite{Agarwal:Ceres:2022} can be used.

% It is worth realising that even if a rigid body model is used, formulation \eqref{eq:dt3d2d} may have somewhat trivial solutions if configurations can be found where the projected 2D point set can fit completely inside of the object segmentation mask as this would lead to a zero loss. It may thus be advantageous to somewhat also consider points outside of the model and make sure that these stay outside of the segmentation mask.
